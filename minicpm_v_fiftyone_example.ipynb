{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/minicpm-v/blob/main/minicpm_v_fiftyone_example.ipynb)\n",
        "\n",
        "\n",
        "# MiniCPM-V Integration with FiftyOne - Example Notebook\n",
        "\n",
        "This notebook demonstrates how to use MiniCPM-V 4.5, a powerful 8B parameter multimodal language model, as a remote source zoo model in FiftyOne.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How to register and load MiniCPM-V as a FiftyOne zoo model\n",
        "- How to use all 6 supported operations:\n",
        "  - Visual Question Answering (VQA)\n",
        "  - Object Detection\n",
        "  - Phrase Grounding\n",
        "  - Image Classification\n",
        "  - Keypoint Detection\n",
        "  - OCR (Optical Character Recognition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "First, let's make sure we have all the necessary dependencies installed and import the required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if not already installed\n",
        "# Uncomment the following lines if needed:\n",
        "# !pip install fiftyone\n",
        "# !pip install torch torchvision\n",
        "# !pip install transformers\n",
        "# !pip install huggingface-hub\n",
        "\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Register and Load MiniCPM-V Model\n",
        "\n",
        "Now let's register the MiniCPM-V model source and load the model. The model will be downloaded on first use (approximately 16GB).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register the MiniCPM-V model source\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/minicpm-v\", \n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "print(\"âœ… MiniCPM-V model source registered successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model (this will download the model on first use)\n",
        "# Note: The download is approximately 16GB and may take some time\n",
        "model = foz.load_zoo_model(\n",
        "    \"openbmb/MiniCPM-V-4_5\",\n",
        "    # install_requirements=True  # Uncomment if you're unsure about dependencies\n",
        ")\n",
        "\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "print(f\"Device: {model.device}\")  # Will show cuda, mps, or cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Sample Dataset\n",
        "\n",
        "Let's load a sample dataset from the FiftyOne zoo to demonstrate the model's capabilities. We'll use the quickstart dataset with a small number of samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample dataset\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"quickstart\", \n",
        "    max_samples=10,  # Using 10 samples for quick demonstration\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Prepare object labels for detection tasks\n",
        "labels_per_sample = dataset.values(\"ground_truth.detections.label\")\n",
        "unique_labels_per_sample = [list(set(labels)) for labels in labels_per_sample]\n",
        "dataset.set_values(\"objects\", unique_labels_per_sample)\n",
        "\n",
        "print(f\"Added 'objects' field with unique labels per sample\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visual Question Answering (VQA)\n",
        "\n",
        "Let's start with VQA to generate natural language descriptions of our images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual Question Answering - Generate descriptions\n",
        "model.operation = \"vqa\"\n",
        "model.prompt = \"Describe this image in detail, including the main subjects, actions, and setting.\"\n",
        "\n",
        "print(\"ðŸ”„ Generating image descriptions...\")\n",
        "dataset.apply_model(model, label_field=\"descriptions\")\n",
        "\n",
        "print(\"âœ… Descriptions generated!\")\n",
        "print(\"\\nSample descriptions:\")\n",
        "for i, sample in enumerate(dataset.head(3)):\n",
        "    print(f\"\\nSample {i+1}: {sample.descriptions[:100]}...\")  # Show first 100 chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can also ask specific questions\n",
        "model.prompt = \"What is the main color scheme in this image?\"\n",
        "\n",
        "print(\"ðŸ”„ Analyzing color schemes...\")\n",
        "dataset.apply_model(model, label_field=\"color_analysis\")\n",
        "\n",
        "print(\"âœ… Color analysis complete!\")\n",
        "print(\"\\nSample color analyses:\")\n",
        "for i, sample in enumerate(dataset.head(3)):\n",
        "    print(f\"\\nSample {i+1}: {sample.color_analysis}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Object Detection\n",
        "\n",
        "Now let's detect and localize objects in the images using bounding boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Object Detection with a predefined list of objects\n",
        "model.operation = \"detect\"\n",
        "model.prompt = ['person', 'car', 'dog', 'cat', 'bicycle', 'traffic light']\n",
        "\n",
        "print(\"ðŸ”„ Detecting objects...\")\n",
        "dataset.apply_model(model, label_field=\"pred_detections\")\n",
        "\n",
        "print(\"âœ… Object detection complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Object Detection using a prompt field from the dataset\n",
        "# This uses the 'objects' field we created earlier\n",
        "model.operation = \"detect\"\n",
        "\n",
        "print(\"ðŸ”„ Detecting objects using prompt field...\")\n",
        "dataset.apply_model(model, label_field=\"pf_detections\", prompt_field=\"objects\")\n",
        "\n",
        "print(\"âœ… Prompt field detection complete!\")\n",
        "print(\"This detection used the unique objects from ground truth for each image\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Phrase Grounding\n",
        "\n",
        "Phrase grounding locates specific regions described by natural language phrases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phrase Grounding - Find regions based on descriptions\n",
        "model.operation = \"phrase_grounding\"\n",
        "\n",
        "print(\"ðŸ”„ Performing phrase grounding using descriptions...\")\n",
        "dataset.apply_model(model, label_field=\"pg_detections\", prompt_field=\"descriptions\")\n",
        "\n",
        "print(\"âœ… Phrase grounding complete!\")\n",
        "print(\"The model located regions based on the generated descriptions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Image Classification\n",
        "\n",
        "Classify images into predefined or open-ended categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification with specific categories\n",
        "model.operation = \"classify\"\n",
        "model.prompt = \"Classify this image into exactly one of the following: indoor, outdoor, people, animals, vehicles, food\"\n",
        "\n",
        "print(\"ðŸ”„ Classifying images...\")\n",
        "dataset.apply_model(model, label_field=\"scene_class\")\n",
        "\n",
        "print(\"âœ… Classification complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-label classification\n",
        "model.prompt = \"Identify all relevant attributes: daytime/nighttime, urban/rural, crowded/empty\"\n",
        "\n",
        "print(\"ðŸ”„ Performing multi-label classification...\")\n",
        "dataset.apply_model(model, label_field=\"attributes\")\n",
        "\n",
        "print(\"âœ… Multi-label classification complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Keypoint Detection\n",
        "\n",
        "Identify key points of interest in images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keypoint Detection\n",
        "model.operation = \"point\"\n",
        "\n",
        "print(\"ðŸ”„ Detecting keypoints...\")\n",
        "dataset.apply_model(model, label_field=\"keypoints\",prompt_field=\"objects\")\n",
        "\n",
        "print(\"âœ… Keypoint detection complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. OCR (Optical Character Recognition)\n",
        "\n",
        "Extract text from images while preserving formatting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OCR - Extract text from images\n",
        "model.operation = \"ocr\"\n",
        "model.prompt = \"Extract all visible text from this image\"\n",
        "\n",
        "print(\"ðŸ”„ Extracting text from images...\")\n",
        "dataset.apply_model(model, label_field=\"extracted_text\")\n",
        "\n",
        "print(\"âœ… OCR complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Custom System Prompts\n",
        "\n",
        "You can customize the system prompt for any operation to specialize the model's behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Custom system prompt for specialized analysis\n",
        "model.operation = \"vqa\"\n",
        "model.system_prompt = \"\"\"You are a photography expert. Analyze images from a technical perspective, \n",
        "commenting on composition, lighting, color balance, and artistic elements. \n",
        "Keep responses concise and professional.\"\"\"\n",
        "\n",
        "model.prompt = \"Analyze the photographic qualities of this image\"\n",
        "\n",
        "print(\"ðŸ”„ Performing technical photography analysis...\")\n",
        "dataset.apply_model(model, label_field=\"photo_analysis\")\n",
        "\n",
        "print(\"âœ… Photography analysis complete!\")\n",
        "\n",
        "# Show a sample analysis\n",
        "sample = dataset.first()\n",
        "if sample.photo_analysis:\n",
        "    print(f\"\\nSample photography analysis:\\n{sample.photo_analysis}\")\n",
        "\n",
        "# Reset to default system prompt\n",
        "model.system_prompt = None  # This will use the default for the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Visualizing Results in FiftyOne App\n",
        "\n",
        "The FiftyOne App provides powerful visualization capabilities for all the predictions we've generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refresh the session to see all new fields\n",
        "fo.launch_app(dataset)\n",
        "\n",
        "\n",
        "print(\"\\nðŸŽ¯ Tips for using the FiftyOne App:\")\n",
        "print(\"  1. Click on samples to see detailed predictions\")\n",
        "print(\"  2. Use the sidebar to toggle different label fields on/off\")\n",
        "print(\"  3. Filter samples based on predictions using the filter bar\")\n",
        "print(\"  4. Compare ground truth with predictions side by side\")\n",
        "print(\"  5. Use the color scheme options to differentiate label types\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important License Note\n",
        "\n",
        "**MiniCPM-V Model License**: The model weights are subject to the [MiniCPM Model License](https://github.com/OpenBMB/MiniCPM-V/blob/main/MiniCPM%20Model%20License.md).\n",
        "\n",
        "**Commercial Use Restrictions:**\n",
        "- Free use allowed for edge devices â‰¤5,000 units or apps with <1M daily active users (registration required)\n",
        "- Other commercial use requires explicit authorization from OpenBMB\n",
        "- Cannot use outputs to enhance other models\n",
        "- See the full license for complete terms and restrictions\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
